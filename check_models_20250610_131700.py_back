#!/usr/bin/env python3
"""
Ollama ëª¨ë¸ í™•ì¸ ë° ì¶”ì²œ ìŠ¤í¬ë¦½íŠ¸
í˜„ì¬ ì„¤ì¹˜ëœ ëª¨ë¸ë“¤ê³¼ ì¶”ì²œ ëª¨ë¸ë“¤ì„ í™•ì¸í•©ë‹ˆë‹¤.
"""

import subprocess
import json
import requests
import sys

def check_ollama_status():
    """Ollama ì„œë²„ ìƒíƒœ í™•ì¸"""
    try:
        response = requests.get("http://localhost:11434/api/tags", timeout=5)
        if response.status_code == 200:
            return True, response.json()
        else:
            return False, None
    except Exception as e:
        return False, str(e)

def get_system_info():
    """ì‹œìŠ¤í…œ ì •ë³´ í™•ì¸"""
    try:
        # GPU ë©”ëª¨ë¦¬ í™•ì¸ (nvidia-smiê°€ ìˆëŠ” ê²½ìš°)
        result = subprocess.run(['nvidia-smi', '--query-gpu=memory.total,memory.used', '--format=csv,noheader,nounits'], 
                              capture_output=True, text=True)
        if result.returncode == 0:
            lines = result.stdout.strip().split('\n')
            gpu_info = []
            for line in lines:
                total, used = line.split(', ')
                gpu_info.append({
                    'total_mb': int(total),
                    'used_mb': int(used),
                    'free_mb': int(total) - int(used)
                })
            return gpu_info
        else:
            return None
    except:
        return None

def recommend_models(gpu_memory_mb=None):
    """GPU ë©”ëª¨ë¦¬ ê¸°ì¤€ ëª¨ë¸ ì¶”ì²œ"""
    models = {
        'entry_level': [
            {'name': 'qwen2.5:7b', 'memory_req': 4500, 'desc': 'ë¹ ë¥¸ ì‘ë‹µ, ê¸°ë³¸ ì„±ëŠ¥'},
            {'name': 'llama3.2:3b', 'memory_req': 3000, 'desc': 'ê²½ëŸ‰í™”, ë‚®ì€ ë©”ëª¨ë¦¬'}
        ],
        'mid_level': [
            {'name': 'qwen2.5:14b', 'memory_req': 9000, 'desc': 'ê· í˜•ì¡íŒ ì„±ëŠ¥'},
            {'name': 'llama3.1:8b', 'memory_req': 5500, 'desc': 'í˜„ì¬ ì‚¬ìš©ì¤‘'},
            {'name': 'gemma2:9b', 'memory_req': 6000, 'desc': 'Google ìµœì‹ '}
        ],
        'high_end': [
            {'name': 'qwen2.5:32b', 'memory_req': 20000, 'desc': 'ê³ ì„±ëŠ¥, ì •í™•ì„± ìš°ìˆ˜'},
            {'name': 'llama3.3:70b', 'memory_req': 42000, 'desc': 'Meta ìµœì‹ , ìµœê³  ì„±ëŠ¥'},
            {'name': 'deepseek-v3:67b', 'memory_req': 40000, 'desc': 'ìµœì‹  DeepSeek, ì½”ë”© ìš°ìˆ˜'}
        ],
        'premium': [
            {'name': 'qwen2.5:72b', 'memory_req': 45000, 'desc': 'ìµœê³  ìˆ˜ì¤€ ì„±ëŠ¥'},
            {'name': 'deepseek-v3:236b', 'memory_req': 150000, 'desc': 'ìµœê³ ê¸‰ ëª¨ë¸ (ìš”êµ¬ì‚¬ì–‘ ê·¹ê³ )'}
        ]
    }
    
    if gpu_memory_mb:
        print(f"ğŸ” GPU ë©”ëª¨ë¦¬: {gpu_memory_mb} MB ê¸°ì¤€ ì¶”ì²œ:")
        for category, model_list in models.items():
            suitable_models = [m for m in model_list if m['memory_req'] <= gpu_memory_mb * 0.8]  # 80% ì—¬ìœ 
            if suitable_models:
                print(f"\nğŸ“‹ {category.upper()}:")
                for model in suitable_models:
                    print(f"  âœ… {model['name']} - {model['desc']} (ë©”ëª¨ë¦¬: {model['memory_req']}MB)")
    else:
        print("ğŸ’¾ GPU ì •ë³´ë¥¼ í™•ì¸í•  ìˆ˜ ì—†ì–´ ì „ì²´ ëª¨ë¸ ëª©ë¡ì„ í‘œì‹œí•©ë‹ˆë‹¤:")
        for category, model_list in models.items():
            print(f"\nğŸ“‹ {category.upper()}:")
            for model in model_list:
                print(f"  â€¢ {model['name']} - {model['desc']} (ë©”ëª¨ë¦¬: {model['memory_req']}MB)")

def main():
    print("ğŸ¤– Dec207Hub ëª¨ë¸ ì—…ê·¸ë ˆì´ë“œ ì²´í¬")
    print("=" * 50)
    
    # Ollama ìƒíƒœ í™•ì¸
    is_running, data = check_ollama_status()
    if not is_running:
        print("âŒ Ollama ì„œë²„ê°€ ì‹¤í–‰ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        print("   ëª…ë ¹ì–´: ollama serve")
        return
    
    print("âœ… Ollama ì„œë²„ ì‹¤í–‰ ì¤‘")
    
    # í˜„ì¬ ì„¤ì¹˜ëœ ëª¨ë¸ í™•ì¸
    if data and 'models' in data:
        print(f"\nğŸ“¦ í˜„ì¬ ì„¤ì¹˜ëœ ëª¨ë¸ ({len(data['models'])}ê°œ):")
        for model in data['models']:
            name = model.get('name', 'Unknown')
            size = model.get('size', 0) / (1024**3)  # GB ë³€í™˜
            print(f"  â€¢ {name} ({size:.1f}GB)")
    
    # ì‹œìŠ¤í…œ ì •ë³´ í™•ì¸
    gpu_info = get_system_info()
    if gpu_info:
        print(f"\nğŸ–¥ï¸ GPU ì •ë³´:")
        for i, gpu in enumerate(gpu_info):
            print(f"  GPU {i}: {gpu['total_mb']}MB ì´ìš©ëŸ‰, {gpu['free_mb']}MB ì‚¬ìš©ê°€ëŠ¥")
        
        # ê°€ì¥ í° GPU ë©”ëª¨ë¦¬ ê¸°ì¤€ ì¶”ì²œ
        max_gpu_memory = max(gpu['free_mb'] for gpu in gpu_info)
        recommend_models(max_gpu_memory)
    else:
        print("\nğŸ–¥ï¸ GPU ì •ë³´ë¥¼ í™•ì¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤ (CPU ëª¨ë“œ ë˜ëŠ” nvidia-smi ì—†ìŒ)")
        recommend_models()
    
    print(f"\nğŸ’¡ ì¶”ì²œ ì—…ê·¸ë ˆì´ë“œ ìˆœì„œ:")
    print(f"  1. qwen2.5:14b (ì¤‘ê¸‰ ì„±ëŠ¥í–¥ìƒ)")
    print(f"  2. qwen2.5:32b (ê³ ê¸‰ ì„±ëŠ¥)")
    print(f"  3. llama3.3:70b (ìµœê³ ê¸‰)")
    
    print(f"\nğŸ”§ ëª¨ë¸ ì„¤ì¹˜ ëª…ë ¹ì–´ ì˜ˆì‹œ:")
    print(f"  ollama pull qwen2.5:14b")
    print(f"  ollama pull qwen2.5:32b")

if __name__ == "__main__":
    main()
