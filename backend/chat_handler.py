# Dec207Hub Backend Chat Handler
# Gemma3-Tools 4B + Gemma3 4B ì¼ë°˜í™”ëœ í• ë£¨ì‹œë„¤ì´ì…˜ ë°©ì§€

import json
import httpx
import logging
import re
from datetime import datetime
from typing import List, Dict, Any, Optional
from config import (
    OLLAMA_BASE_URL, DEFAULT_MODEL, FALLBACK_MODEL, HTTP_TIMEOUT, 
    MAX_CONVERSATION_HISTORY, MAX_CONTEXT_MESSAGES, MAX_MESSAGE_LENGTH,
    AI_TEMPERATURE, AI_TOP_P, AI_REPEAT_PENALTY, AI_MAX_NEW_TOKENS,
    ENABLE_MCP, ENABLE_FUNCTION_CALLING, ENABLE_FACT_CHECK, 
    FAST_RESPONSE_MODE, RESPONSE_TIMEOUT
)

logger = logging.getLogger(__name__)

async def chat_with_ollama(message: str, model: str = DEFAULT_MODEL, 
                          conversation_history: List[Dict] = None,
                          enable_tools: bool = True) -> str:
    """Gemma3-Tools 4B ì±„íŒ… + ì¼ë°˜í™”ëœ í• ë£¨ì‹œë„¤ì´ì…˜ ë°©ì§€"""
    try:
        timeout = RESPONSE_TIMEOUT if FAST_RESPONSE_MODE else HTTP_TIMEOUT
        async with httpx.AsyncClient(timeout=timeout) as client:
            # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
            context_prompt = build_safe_context_prompt(conversation_history)
            
            # ì¼ë°˜í™”ëœ ì•ˆì „ í”„ë¡¬í”„íŠ¸
            enhanced_prompt = build_general_safety_prompt(context_prompt, message)
            
            # ì•ˆì „í•œ ì‘ë‹µìš© í˜ì´ë¡œë“œ
            payload = build_safe_payload(model, enhanced_prompt)
            
            logger.info(f"Gemma3-Tools 4B ìš”ì²­: {message[:30]}...")
            response = await client.post(f"{OLLAMA_BASE_URL}/api/chat", json=payload)
            
            if response.status_code == 200:
                data = response.json()
                
                # ì‘ë‹µ ì¶”ì¶œ ë° ê²€ì¦
                ai_response = extract_and_validate_response(data, message)
                
                logger.info(f"Gemma3-Tools 4B ì‘ë‹µ ì™„ë£Œ: {ai_response[:50]}...")
                return ai_response
            else:
                logger.error(f"Ollama API ì˜¤ë¥˜: {response.status_code}")
                return await fallback_chat(message, conversation_history)
                
    except httpx.TimeoutException:
        logger.warning("Gemma3-Tools 4B ì‘ë‹µ ì‹œê°„ ì´ˆê³¼ - ë°±ì—… ëª¨ë¸ ì‚¬ìš©")
        return await fallback_chat(message, conversation_history)
    except Exception as e:
        logger.error(f"Gemma3-Tools 4B ì˜¤ë¥˜: {str(e)}")
        return "AI ì—°ê²° ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."

def build_safe_context_prompt(conversation_history: List[Dict]) -> str:
    """ì•ˆì „í•œ ì»¨í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ êµ¬ì„±"""
    if not conversation_history:
        return ""
    
    # ìµœê·¼ ëŒ€í™”ë§Œ ê°„ë‹¨íˆ í¬í•¨ (4B ëª¨ë¸ìš©)
    recent_history = conversation_history[-2:]
    context_lines = []
    
    for msg in recent_history:
        role = "ì‚¬ìš©ì" if msg.get('role') == 'user' else "AI"
        content = msg.get('content', '')[:150]
        context_lines.append(f"{role}: {content}")
    
    return "ì´ì „ ëŒ€í™”:\n" + "\n".join(context_lines) + "\n\n" if context_lines else ""

def build_general_safety_prompt(context_prompt: str, message: str) -> str:
    """ì¼ë°˜í™”ëœ ì•ˆì „ í”„ë¡¬í”„íŠ¸"""
    return f"""ì •í™•ì„±ì„ ìµœìš°ì„ ìœ¼ë¡œ í•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

**ì‘ë‹µ ì›ì¹™:**
1. ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ì •ë³´ë‚˜ ìµœì‹  ì •ë³´ ì¡°ì‘ ê¸ˆì§€
2. ì‚¬ì‹¤ê³¼ ì˜ê²¬ì„ ëª…í™•íˆ êµ¬ë¶„

{context_prompt}ì§ˆë¬¸: {message}

ì •í™•í•œ ë‹µë³€:"""

def build_safe_payload(model: str, prompt: str) -> Dict[str, Any]:
    """ì•ˆì „í•œ ì‘ë‹µìš© í˜ì´ë¡œë“œ (4B ëª¨ë¸ ìµœì í™”)"""
    return {
        "model": model,
        "messages": [{"role": "user", "content": prompt}],
        "stream": False,
        "options": {
            "temperature": 0.2,  # ë‚®ì€ ì˜¨ë„ë¡œ ì¼ê´€ì„± í™•ë³´
            "top_p": 0.95,         # í† í° ì„ íƒ ë²”ìœ„ ì¶•ì†Œ
            "repeat_penalty": 1.2,
            "num_predict": 2000,   # 4B ëª¨ë¸ìš© í† í° ìˆ˜
            "num_ctx": 4096,      # 4B ëª¨ë¸ìš© ì»¨í…ìŠ¤íŠ¸
        }
    }

def extract_and_validate_response(data: Dict[str, Any], original_message: str) -> str:
    """ì‘ë‹µ ì¶”ì¶œ ë° ì¼ë°˜í™”ëœ ê²€ì¦"""
    ai_response = data.get("message", {}).get("content", "ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.").strip()
    
    # ê¸°ë³¸ ì •ë¦¬
    ai_response = re.sub(r'\n{3,}', '\n\n', ai_response)
    ai_response = ai_response.strip()
    
    # ì¼ë°˜í™”ëœ í• ë£¨ì‹œë„¤ì´ì…˜ ê²€ì¦
    ai_response = detect_general_hallucinations(ai_response)
    
    return ai_response

def detect_general_hallucinations(response: str) -> str:
    """ì¼ë°˜í™”ëœ í• ë£¨ì‹œë„¤ì´ì…˜ íŒ¨í„´ ê°ì§€"""
    
    # ì¼ë°˜ì ì¸ ìœ„í—˜ íŒ¨í„´ë“¤
    dangerous_patterns = [
#        (r'ìµœê·¼ì—?\s*(ë°œí‘œ|ê³µê°œ|ì¶œì‹œ|ë°œê²¬)', 'ìµœì‹  ì •ë³´'),
#        (r'2025ë…„.*ì´í›„', 'ë¯¸ë˜ ì •ë³´'),
#        (r'ê³µì‹.*ë°œí‘œ.*í–ˆìŠµë‹ˆë‹¤', 'ê³µì‹ ë°œí‘œ'),
#        (r'ì—°êµ¬.*ê²°ê³¼.*ë³´ì—¬ì¤ë‹ˆë‹¤', 'ì—°êµ¬ ê²°ê³¼'),
#        (r'ì „ë¬¸ê°€.*ë§í–ˆìŠµë‹ˆë‹¤', 'ì „ë¬¸ê°€ ì¸ìš©'),
#        (r'í™•ì‹¤í•œ?\s*ì •ë³´.*ìˆìŠµë‹ˆë‹¤', 'í™•ì‹¤ì„± ê³¼ì¥'),
#        (r'ìƒˆë¡œìš´?\s*(ê¸°ìˆ |ì œí’ˆ|ì„œë¹„ìŠ¤)', 'ì‹ ê¸°ìˆ  ì •ë³´')
    ]
    
    found_issues = []
    for pattern, issue_type in dangerous_patterns:
        if re.search(pattern, response, re.IGNORECASE):
            found_issues.append(issue_type)
    
    if found_issues:
        warning = f"âš ï¸ **ì •í™•ì„± ì£¼ì˜**: ì´ ë‹µë³€ì—ëŠ” ë¶ˆí™•ì‹¤í•œ ì •ë³´ê°€ í¬í•¨ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n"
        response = warning + response + "\n\nğŸ’¡ **ê¶Œì¥**: ì¤‘ìš”í•œ ì •ë³´ëŠ” ê³µì‹ ì†ŒìŠ¤ì—ì„œ ì¬í™•ì¸í•´ì£¼ì„¸ìš”."
    
    return response

async def fallback_chat(message: str, conversation_history: List[Dict] = None) -> str:
    """ë°±ì—… ëª¨ë¸ë¡œ ì•ˆì „í•œ ì „í™˜"""
    try:
        logger.info(f"ë°±ì—… ëª¨ë¸ {FALLBACK_MODEL} ì‚¬ìš©")
        
        safe_prompt = f"""ì •í™•í•œ ì •ë³´ë§Œ ì œê³µí•˜ì„¸ìš”. í™•ì‹¤í•˜ì§€ ì•Šìœ¼ë©´ "ì˜ ëª¨ë¥´ê² ìŠµë‹ˆë‹¤"ë¼ê³  ë‹µë³€í•˜ì„¸ìš”.

ì§ˆë¬¸: {message}

ë‹µë³€:"""
        
        payload = {
            "model": FALLBACK_MODEL,
            "messages": [{"role": "user", "content": safe_prompt}],
            "stream": False,
            "options": {
                "temperature": 0.05,
                "num_predict": 2000,
            }
        }
        
        async with httpx.AsyncClient(timeout=15.0) as client:
            response = await client.post(f"{OLLAMA_BASE_URL}/api/chat", json=payload)
            if response.status_code == 200:
                data = response.json()
                return data.get("message", {}).get("content", "ë°±ì—… ì‘ë‹µ ì‹¤íŒ¨").strip()
            
    except Exception as e:
        logger.error(f"ë°±ì—… ëª¨ë¸ ì‹¤íŒ¨: {str(e)}")
    
    return "ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ AI ì„œë¹„ìŠ¤ì— ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."

def get_model_status() -> Dict[str, Any]:
    """ëª¨ë¸ ìƒíƒœ ì •ë³´"""
    return {
        "primary_model": DEFAULT_MODEL,
        "fallback_model": FALLBACK_MODEL,
        "anti_hallucination": True,
        "general_safety": True,
        "fact_check": ENABLE_FACT_CHECK
    }

logger.info("ğŸ›¡ï¸ Gemma3-Tools 4B ì¼ë°˜í™”ëœ ì•ˆì „ Chat Handler ë¡œë“œ ì™„ë£Œ")
logger.info(f"ğŸš€ ë©”ì¸ ëª¨ë¸: {DEFAULT_MODEL}")
logger.info(f"ğŸ”§ ë°±ì—… ëª¨ë¸: {FALLBACK_MODEL}")
logger.info("ğŸ›¡ï¸ ì¼ë°˜í™”ëœ í• ë£¨ì‹œë„¤ì´ì…˜ ë°©ì§€ ì‹œìŠ¤í…œ í™œì„±í™”")
